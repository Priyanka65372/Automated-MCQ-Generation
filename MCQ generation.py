# -*- coding: utf-8 -*-
"""just_need_to_add_gui_interface.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ArsHydCwf8vDpJNcK2J40qAHsfD7GDBU
"""

!pip install transformers

!pip install torch
!pip install tensorflow

!pip install keybert

!pip install -U nltk
!pip install -U pywsd

!pip install gensim
!pip install git+https://github.com/boudinfl/pke.git

# Commented out IPython magic to ensure Python compatibility.
# %config Completer.use_jedi=False

from transformers import pipeline
from transformers import BartForConditionalGeneration, BartTokenizer
from keybert import KeyBERT

import tensorflow as tf
import torch
import numpy as np
import pandas as pd
import re
import matplotlib.pyplot as plt
import random
import copy
import sys

from nltk import wsd
import pandas as pd
import numpy as np
import nltk
from nltk.corpus import wordnet as wn
from spacy.cli import download
from spacy import load
import warnings
from nltk.tokenize import sent_tokenize
nltk.download('wordnet')

nltk.download('averaged_perceptron_tagger')

import requests
import json
import re
import random
from pywsd.similarity import max_similarity
from pywsd.lesk import adapted_lesk
from pywsd.lesk import simple_lesk
from pywsd.lesk import cosine_lesk
from nltk.corpus import wordnet as wn
from nltk.tokenize import word_tokenize
from transformers import GPT2Tokenizer, GPT2LMHeadModel

from transformers import T5ForConditionalGeneration, T5Tokenizer

import transformers

model= BartForConditionalGeneration.from_pretrained("facebook/bart-large-cnn")
tokenizer= BartTokenizer.from_pretrained("facebook/bart-large-cnn")

article="""IN July 1976, my wife Mary, son Jonathan, 6, daughter Suzanne,
7, and I set sail from Plymouth, England, to duplicate the roundthe-world voyage made 200 years earlier by Captain James Cook.
For the longest time, Mary and I — a 37-year-old businessman —
had dreamt of sailing in the wake of the famous explorer, and
for the past 16 years we had spent all our leisure time honing
our seafaring skills in British waters.
Our boat Wavewalker, a 23 metre, 30 ton wooden-hulled
beauty, had been professionally built, and we had spent months
fitting it out and testing it in the roughest weather we could find.
The first leg of our planned three-year, 105,000 kilometre
journey passed pleasantly as we sailed down the west coast of
Africa to Cape Town. There, before heading east, we took on two
crewmen — American Larry Vigil and Swiss Herb Seigler — to
help us tackle one of the world’s roughest seas, the southern
Indian Ocean.
On our second day out of Cape Town, we began to encounter
strong gales. For the next few weeks, they blew continuously.
Gales did not worry me; but the size of the waves was alarming —
up to 15 metres, as high as our main mast.
December 25 found us 3,500 kilometres east of Cape Town.
Despite atrocious weather, we had a wonderful holiday complete
with a Christmas tree. New Year’s Day saw no improvement in
the weather, but we reasoned that it had to change soon. And it
did change — for the worse.
At dawn on January 2, the waves were gigantic. We were
sailing with only a small storm jib and were still making eight
knots. As the ship rose to the top of each wave we could see
endless enormous seas rolling towards us, and the screaming
of the wind and spray was painful to the ears. To slow the boat
down, we dropped the storm jib and lashed a heavy mooring
rope in a loop across the stern. Then we double-lashed
everything, went through our life-raft drill, attached lifelines,
donned oilskins and life jackets — and waited.
The first indication of impending disaster came at about
6 p.m., with an ominous silence. The wind dropped, and the
sky immediately grew dark. Then came a growing roar, and an
enormous cloud towered aft of the ship. With horror, I realised
that it was not a cloud, but a wave like no other I had ever seen.
It appeared perfectly vertical and almost twice the height of the
other waves, with a frightful breaking crest.
The roar increased to a thunder as the stern moved up the
face of the wave, and for a moment I thought we might ride over
it. But then a tremendous explosion shook the deck. A torrent
of green and white water broke over the ship, my head smashed
into the wheel and I was aware of flying overboard and sinking
below the waves. I accepted my approaching death, and as I
was losing consciousness, I felt quite peaceful.
Unexpectedly, my head popped out of the water. A few metres
away, Wavewalker was near capsizing, her masts almost
horizontal. Then a wave hurled her upright, my lifeline jerked
taut, I grabbed the guard rails and sailed through the air into
Wavewalker’s main boom. Subsequent waves tossed me around
the deck like a rag doll. My left ribs cracked; my mouth filled
with blood and broken teeth. Somehow, I found the wheel, lined
up the stern for the next wave and hung on.
Water, Water, Everywhere. I could feel that the ship had water
below, but I dared not abandon the wheel to investigate. Suddenly,
the front hatch was thrown open and Mary appeared. “We’re sinking!”
she screamed. “The decks are smashed; we’re full of water.”
“Take the wheel”, I shouted as I scrambled for the hatch.
Larry and Herb were pumping like madmen. Broken timbers
hung at crazy angles, the whole starboard side bulged inwards;
clothes, crockery, charts, tins and toys sloshed about in deep water.
I half-swam, half-crawled into the children’s cabin. “Are you
all right?” I asked. “Yes,” they answered from an upper bunk.
“But my head hurts a bit,” said Sue, pointing to a big bump
above her eyes. I had no time to worry about bumped heads.
After finding a hammer, screws and canvas, I struggled back
on deck. With the starboard side bashed open, we were taking
water with each wave that broke over us. If I couldn’t make
some repairs, we would surely sink.
Somehow I managed to stretch canvas and secure waterproof
hatch covers across the gaping holes. Some water continued to
stream below, but most of it was now being deflected over the side.
More problems arose when our hand pumps started to block
up with the debris floating around the cabins and the electric
pump short-circuited. The water level rose threateningly. Back
on deck I found that our two spare hand pumps had been
wrenched overboard — along with the forestay sail, the jib, the
dinghies and the main anchor.
Then I remembered we had another electric pump under
the chartroom floor. I connected it to an out-pipe, and was
thankful to find that it worked.
The night dragged on with an endless, bitterly cold routine
of pumping, steering and working the radio. We were getting no
replies to our Mayday calls — which was not surprising in this
remote corner of the world.
Sue’s head had swollen alarmingly; she had two enormous
black eyes, and now she showed us a deep cut on her arm.
When I asked why she hadn’t made more of her injuries before
this, she replied, “I didn’t want to worry you when you were
trying to save us all.” """

def summarize(text, maxSummarylength=1000):
  inputs= tokenizer.encode("summarize: "+text,
                           return_tensors="pt",
                           max_length=1024, truncation=True)
  summary_ids= model.generate(inputs, max_length=maxSummarylength,
                              min_length= int(maxSummarylength/5),
                              length_penalty=10.0,
                              num_beams= 4, early_stopping= True)
  summary= tokenizer.decode(summary_ids[0], skip_special_tokens= True)
  return summary

def split_text_into_pieces(text, max_tokens=2000, overlapPercent=10):
  tokens=tokenizer.tokenize(text)
  overlap_tokens=int(max_tokens * overlapPercent/100)
  pieces=[tokens[i:i + max_tokens]
          for i in range(0, len(tokens),
                         max_tokens - overlap_tokens)]
  text_pieces= [tokenizer.decode(
      tokenizer.convert_tokens_to_ids(piece),
      skip_special_tokens= True) for piece in pieces]

  return text_pieces

def recursive_summarize(text, max_length=1000, recursionLevel=0):
    recursionLevel=recursionLevel+1
    print("######### Recursion level: ",
          recursionLevel,"\n\n######### ")
    tokens = tokenizer.tokenize(text)
    expectedCountOfChunks = len(tokens)/max_length
    max_length=int(len(tokens)/expectedCountOfChunks)+2

    # Break the text into pieces of max_length
    pieces = split_text_into_pieces(text, max_tokens=max_length)

    print("Number of pieces: ", len(pieces))
    # Summarize each piece
    summaries=[]
    k=0
    for k in range(0, len(pieces)):
        piece=pieces[k]
        print("****************************************************")
        print("Piece:",(k+1)," out of ", len(pieces), "pieces")
        print(piece, "\n")
        summary =summarize(piece, maxSummarylength=max_length/3*2)
        print("SUMNMARY: ", summary)
        summaries.append(summary)
        print("****************************************************")

    concatenated_summary = ' '.join(summaries)

    tokens = tokenizer.tokenize(concatenated_summary)

    if len(tokens) > max_length:
        # If the concatenated_summary is too long, repeat the process
        print("############# GOING RECURSIVE ##############")
        return recursive_summarize(concatenated_summary,
                                   max_length=max_length,
                                   recursionLevel=recursionLevel)
    else:
      # Concatenate the summaries and summarize again
        final_summary=concatenated_summary
        if len(pieces)>1:
            final_summary = summarize(concatenated_summary,
                                  maxSummarylength=max_length)
        return final_summary

summary= recursive_summarize(article)
print("\n%%%%%%%%%%%%%%%%%%\n")
print("Final summary: ", summary)

model_2= KeyBERT(model="distilbert-base-nli-mean-tokens")

# Commented out IPython magic to ensure Python compatibility.
# #more relevant
# %%time
# def keyword_extract(article):
#   keywords=model_2.extract_keywords(article,
#                         top_n=80,
#                         keyphrase_ngram_range=(1, 1), #can also generate phrases of more than 1 word (1,2), (1,3)
#                         stop_words="english")
#   return (keywords)

original_keywords=keyword_extract(article)
for i in original_keywords:
  print(i)

required_keywords=[i[0] for i in original_keywords]
print(required_keywords)

filtered_keywords=set()
for keyword in required_keywords:
  if(keyword.lower() in summary.lower()):
    filtered_keywords.add(keyword)

print(filtered_keywords)

original_keywords_2=keyword_extract(summary)
filtered_keyword_2=[i[0] for i in original_keywords_2]
filtered_keyword_2=set(filtered_keyword_2)
print(filtered_keyword_2)

nltk.download('omw-1.4')
nltk.download('wordnet')
nltk.download('wordnet2022')
nlp = load('en_core_web_sm')

! cp -rf /usr/share/nltk_data/corpora/wordnet2022 /usr/share/nltk_data/corpora/wordnet # temp fix for lookup error.

# Automatic POS Tagging & Lesk with spacy

POS_MAP = {
    'VERB': wn.VERB,
    'NOUN': wn.NOUN,
    'PROPN': wn.NOUN
}


def lesk(doc, word):
    found = False
    for token in doc:
        if token.text.lower() == word.lower():
            word = token
            found = True
            break
    if not found:
        #warning.warn(f'Word \"{word}\" does not appear in the document: {doc.text}.')
        return None
    pos = POS_MAP.get(word.pos_, False)
    if not pos:
        #warnings.warn(f'POS tag for {word.text} not found in wordnet. Falling back to default Lesk behaviour.')
        return None
    args = [c.text for c in doc], word.text
    kwargs = dict(pos=pos)
    return wsd.lesk(*args, **kwargs)

nltk.download('punkt')

def corresponding_sentences(text,filtered_keywords):
  sentences=sent_tokenize(text)      #Sentence tokenization
  corr_sentences=dict()
  for keyword in filtered_keywords:
    for sentence in sentences:
      if keyword.lower() in sentence.lower():
        corr_sentences.update({keyword:sentence})  #if keyword is found in the sentence then the dictionary is updated with the corresponding key value pair
  return(corr_sentences)

keyword_sentences=corresponding_sentences(summary, filtered_keywords)
for i,j in keyword_sentences.items():
  print(i,": ",j)

keyword_sentences_2=corresponding_sentences(summary, filtered_keyword_2)
for i,j in keyword_sentences_2.items():
  print(i,": ",j)

def get_wordsense(sent, word):
    word = word.lower()

    if len(word.split()) > 0:
        word = word.replace(" ", "_")

    synsets = wn.synsets(word, 'n')

    if synsets:
        wup = max_similarity(sent, word, 'wup', pos='n')
        adapted_lesk_output = adapted_lesk(sent, word, pos='n')
        valid_synsets = [synset for synset in (wup, adapted_lesk_output) if synset in synsets]
        if not valid_synsets:
            # Default to the first synset or implement other fallback logic
            return synsets[0]
        else:
            # If both are valid, choose the one that appears first in the list
            # Or implement a more sophisticated selection mechanism
            return valid_synsets[0]
    else:
        return None

def get_distractors_wordnet(syn, word):
    distractors = []
    word = word.lower()
    orig_word = word

    if len(word.split()) > 0:
        word = word.replace(" ", "_")

    hypernym = syn.hypernyms()

    if len(hypernym) == 0:
        return distractors

    for item in hypernym[0].hyponyms():
        name = item.lemmas()[0].name()

        if name == orig_word:
            continue

        name = name.replace("_", " ")
        name = " ".join(w.capitalize() for w in name.split())

        if name is not None and name not in distractors:
            distractors.append(name)

    return distractors

def get_distractors_conceptnet(word):
    word = word.lower()
    original_word = word

    if len(word.split()) > 0:
        word = word.replace(" ", "_")

    distractor_list = []
    url = "http://api.conceptnet.io/query?node=/c/en/%s/n&rel=/r/PartOf&start=/c/en/%s&limit=5" % (word, word)
    obj = requests.get(url).json()

    for edge in obj['edges']:
        link = edge['end']['term']

        url2 = "http://api.conceptnet.io/query?node=%s&rel=/r/PartOf&end=%s&limit=10" % (link, link)
        obj2 = requests.get(url2).json()

        for edge in obj2['edges']:
            word2 = edge['start']['label']

            if word2 not in distractor_list and original_word.lower() not in word2.lower():
                distractor_list.append(word2)

    return distractor_list

key_distractor_list = {}

for keyword, sentence in keyword_sentences.items():
    wordsense = get_wordsense(sentence, keyword)
    if wordsense:
        distractors = get_distractors_wordnet(wordsense, keyword)
        if len(distractors) == 0:
            distractors = get_distractors_conceptnet(keyword)
        if len(distractors) != 0:
            key_distractor_list[keyword] = distractors
    else:
        distractors = get_distractors_conceptnet(keyword)
        if len(distractors) != 0:
            key_distractor_list[keyword] = distractors

for i,j in key_distractor_list.items():
  print(i, ": ",j)

key_distractor_list_2 = {}

for keyword, sentence in keyword_sentences_2.items():
    #wordsense = lesk(nlp(sentence), keyword)
    wordesense = get_wordsense(sentence, keyword)
    if wordsense:
        distractors = get_distractors_wordnet(wordsense, keyword)
        if len(distractors) == 0:
            distractors = get_distractors_conceptnet(keyword)
        if len(distractors) != 0:
            key_distractor_list[keyword] = distractors
    else:
        distractors = get_distractors_conceptnet(keyword)
        if len(distractors) != 0:
            key_distractor_list_2[keyword] = distractors

for i,j in key_distractor_list_2.items():
  print(i, ": ",j)

# def combine_distractor_sources(keyword, sentence):
#     # Start with WordNet and ConceptNet distractors
#     wordsense = get_wordsense(sentence, keyword)  # Assuming this function exists
#     if wordsense:
#         distractors = get_distractors_wordnet(wordsense, keyword)
#     else:
#         distractors = get_distractors_conceptnet(keyword)

#     # Enhance with GPT-2 generated distractors
#     gpt2_distractors = generate_distractors_gpt2(keyword)
#     all_distractors = distractors + gpt2_distractors

#     # Post-process to remove duplicates and ensure relevance
#     # This could be as simple as converting to a set and back to a list, if appropriate
#     unique_distractors = list(set(all_distractors))

#     # Optionally, further refine or filter the distractors here
#     # For example, remove those that are too similar to the keyword or to each other,
#     # or manually review them if necessary

#     return unique_distractors

# # Example usage
# keyword = "eclipse"
# sentence = "A solar eclipse occurs when the moon gets between Earth and the sun."
# distractors = combine_distractor_sources(keyword, sentence)
# print(distractors)

model_bart = BartForConditionalGeneration.from_pretrained("facebook/bart-large-cnn")
tokenizer_bart = BartTokenizer.from_pretrained("facebook/bart-large-cnn")

def generate_distractors_bart(sentence):
    inputs = tokenizer_bart([sentence], max_length=1024, return_tensors="pt", truncation=True)
    # Adjust min_length to be smaller or equal to max_length
    outputs = model_bart.generate(**inputs, max_length=100, min_length=10, num_return_sequences=4, early_stopping=True)
    distractors = [tokenizer_bart.decode(output, skip_special_tokens=True) for output in outputs]
    return distractors

def get_wordsense(sent, word):
    word = word.lower()

    if len(word.split()) > 0:
        word = word.replace(" ", "_")

    synsets = wn.synsets(word, 'n')

    if synsets:
        wup = max_similarity(sent, word, 'wup', pos='n')
        adapted_lesk_output = adapted_lesk(sent, word, pos='n')
        valid_synsets = [synset for synset in (wup, adapted_lesk_output) if synset in synsets]
        if not valid_synsets:
            # Default to the first synset or implement other fallback logic
            return synsets[0]
        else:
            # If both are valid, choose the one that appears first in the list
            # Or implement a more sophisticated selection mechanism
            return valid_synsets[0]
    else:
        return None

key_distractor_list_3 = {}

for keyword, sentence in keyword_sentences.items():
    wordsense = get_wordsense(sentence, keyword)
    if wordsense:
        distractors = get_distractors_wordnet(wordsense, keyword)
        if len(distractors) == 0:
            distractors = get_distractors_conceptnet(keyword)
        if len(distractors) != 0:
            # Generate distractors using BART
            bart_distractors = generate_distractors_bart(sentence)
            distractors.extend(bart_distractors)
            key_distractor_list_3[keyword] = distractors
    else:
        distractors = get_distractors_conceptnet(keyword)
        if len(distractors) != 0:
            # Generate distractors using BART
            bart_distractors = generate_distractors_bart(sentence)
            distractors.extend(bart_distractors)
            key_distractor_list_3[keyword] = distractors

for i,j in key_distractor_list_3.items():
  print(i, ": ",j)

print(list(keyword_sentences.keys()))

model_name_2 = "t5-small"
tokenizer = T5Tokenizer.from_pretrained(model_name_2)
model_4 = T5ForConditionalGeneration.from_pretrained(model_name_2)

def generate_mcq(keyword_sentences, key_distractor_list):
    index = 1
    for keyword, sentence in keyword_sentences.items():
        # Check if keyword has distractors; if not, skip it
        if keyword not in key_distractor_list:
            print(f"Skipping '{keyword}': No distractors found.")
            continue  # Skip this keyword

        pattern = re.compile(re.escape(keyword), re.IGNORECASE)
        output = pattern.sub("_______", sentence)
        print(f"{index}) {output}")

        distractors = key_distractor_list[keyword]
        choices = [keyword.capitalize()] + distractors
        top4choices = choices[:4]
        random.shuffle(top4choices)
        optionchoices = ['a', 'b', 'c', 'd']
        for idx, choice in enumerate(top4choices):
            print(f"\t{optionchoices[idx]}) {choice}")
        print("\nMore options:", choices[4:], "\n")
        index += 1

generate_mcq(keyword_sentences, key_distractor_list)

generate_mcq(keyword_sentences_2, key_distractor_list_2)

generate_mcq(keyword_sentences, key_distractor_list_3)

#Fall back mechanism not very apt
def generate_distractors_with_fallback(sentence, keyword):
    # Attempt to generate distractors using BART
    try:
        bart_distractors = generate_distractors_bart(sentence)
        if len(bart_distractors) > 0:
            return bart_distractors
    except Exception as e:
        print(f"Error generating distractors with BART for keyword '{keyword}': {e}")

    # If BART fails or doesn't generate enough distractors, fallback to WordNet and ConceptNet
    wordsense = get_wordsense(sentence, keyword)
    if wordsense:
        distractors = get_distractors_wordnet(wordsense, keyword)
        if len(distractors) == 0:
            distractors = get_distractors_conceptnet(keyword)
        return distractors
    else:
        return get_distractors_conceptnet(keyword)


key_distractor_list_4 = {}

for keyword, sentence in keyword_sentences.items():
    distractors = generate_distractors_with_fallback(sentence, keyword)
    if len(distractors) != 0:
        key_distractor_list_4[keyword] = distractors

for i,j in key_distractor_list_4.items():
  print(i, ": ",j)

